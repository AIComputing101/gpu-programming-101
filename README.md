# GPU Programming 101 ğŸš€

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CUDA](https://img.shields.io/badge/CUDA-12.9.1-76B900?logo=nvidia)](https://developer.nvidia.com/cuda-toolkit)
[![ROCm](https://img.shields.io/badge/ROCm-7.0-red?logo=amd)](https://rocmdocs.amd.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?logo=docker)](https://www.docker.com/)
[![Examples](https://img.shields.io/badge/Examples-71-green)](modules/)
[![CI](https://img.shields.io/badge/CI-GitHub%20Actions-2088FF?logo=github-actions)](https://github.com/features/actions)

**A comprehensive, hands-on educational project for mastering GPU programming with CUDA and HIP**

*From beginner fundamentals to production-ready optimization techniques*

## ğŸ“‘ Table of Contents

- [ğŸ“‹ Project Overview](#-project-overview)
- [ğŸ—ï¸ GPU Programming Architecture](#ï¸-gpu-programming-architecture)
- [âœ¨ Key Features](#-key-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ¯ Learning Path](#-learning-path)
- [ğŸ“š Modules](#-modules)
- [ğŸ› ï¸ Prerequisites](#ï¸-prerequisites)
- [ğŸ³ Docker Development](#-docker-development)
- [ğŸ”§ Build System](#-build-system)
- [ğŸ“Š Performance Expectations](#-performance-expectations)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸ“– Documentation](#-documentation)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## ğŸ“‹ Project Overview

**GPU Programming 101** is a complete educational resource for learning modern GPU programming. This project provides:

- **9 comprehensive modules** covering beginner to expert topics
- **71 working code examples** in both CUDA and HIP
- **Cross-platform support** for NVIDIA and AMD GPUs  
- **Production-ready development environment** with Docker
- **Professional tooling** including profilers, debuggers, and CI/CD

Perfect for students, researchers, and developers looking to master GPU computing.

## ğŸ—ï¸ GPU Programming Architecture

Understanding how GPU programming works from high-level code to hardware execution is crucial for effective GPU development. This section provides a comprehensive overview of the CUDA and HIP ROCm software-hardware stack.

### Architecture Overview Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                APPLICATION LAYER                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  High-Level Code (C++/CUDA/HIP)                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   CUDA C++ Code     â”‚    â”‚    HIP C++ Code     â”‚    â”‚   OpenCL/SYCL       â”‚    â”‚
â”‚  â”‚   (.cu files)       â”‚    â”‚   (.hip files)      â”‚    â”‚   (Cross-platform)   â”‚    â”‚
â”‚  â”‚                     â”‚    â”‚                     â”‚    â”‚                     â”‚    â”‚
â”‚  â”‚ __global__ kernels  â”‚    â”‚ __global__ kernels  â”‚    â”‚ kernel functions    â”‚    â”‚
â”‚  â”‚ cudaMalloc()        â”‚    â”‚ hipMalloc()         â”‚    â”‚ clCreateBuffer()    â”‚    â”‚
â”‚  â”‚ cudaMemcpy()        â”‚    â”‚ hipMemcpy()         â”‚    â”‚ clEnqueueNDRange()  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              COMPILATION LAYER                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Compiler Frontend                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      NVCC           â”‚    â”‚      HIP Clang      â”‚    â”‚    LLVM/Clang       â”‚    â”‚
â”‚  â”‚  (NVIDIA Compiler)  â”‚    â”‚   (AMD Compiler)    â”‚    â”‚   (Open Standard)   â”‚    â”‚
â”‚  â”‚                     â”‚    â”‚                     â”‚    â”‚                     â”‚    â”‚
â”‚  â”‚ â€¢ Parse CUDA syntax â”‚    â”‚ â€¢ Parse HIP syntax  â”‚    â”‚ â€¢ Parse OpenCL/SYCL â”‚    â”‚
â”‚  â”‚ â€¢ Host/Device split â”‚    â”‚ â€¢ Host/Device split â”‚    â”‚ â€¢ Generate SPIR-V   â”‚    â”‚
â”‚  â”‚ â€¢ Generate PTX      â”‚    â”‚ â€¢ Generate GCN ASM  â”‚    â”‚ â€¢ Target backends   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INTERMEDIATE REPRESENTATION                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚        PTX          â”‚    â”‚      GCN ASM        â”‚    â”‚      SPIR-V         â”‚    â”‚
â”‚  â”‚ (Parallel Thread    â”‚    â”‚  (Graphics Core     â”‚    â”‚  (Standard Portable â”‚    â”‚
â”‚  â”‚  Execution)         â”‚    â”‚   Next Assembly)    â”‚    â”‚   IR - Vulkan)      â”‚    â”‚
â”‚  â”‚                     â”‚    â”‚                     â”‚    â”‚                     â”‚    â”‚
â”‚  â”‚ â€¢ Virtual ISA       â”‚    â”‚ â€¢ AMD GPU ISA       â”‚    â”‚ â€¢ Cross-platform    â”‚    â”‚
â”‚  â”‚ â€¢ Device agnostic   â”‚    â”‚ â€¢ RDNA/CDNA arch    â”‚    â”‚ â€¢ Vendor neutral    â”‚    â”‚
â”‚  â”‚ â€¢ JIT compilation   â”‚    â”‚ â€¢ Direct execution  â”‚    â”‚ â€¢ Multiple targets  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               DRIVER LAYER                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚    CUDA Driver      â”‚    â”‚     ROCm Driver     â”‚    â”‚   OpenCL Driver     â”‚    â”‚
â”‚  â”‚                     â”‚    â”‚                     â”‚    â”‚                     â”‚    â”‚
â”‚  â”‚ â€¢ PTX â†’ SASS JIT    â”‚    â”‚ â€¢ GCN â†’ Machine     â”‚    â”‚ â€¢ SPIR-V â†’ Native   â”‚    â”‚
â”‚  â”‚ â€¢ Memory management â”‚    â”‚ â€¢ Memory management â”‚    â”‚ â€¢ Memory management â”‚    â”‚
â”‚  â”‚ â€¢ Kernel launch     â”‚    â”‚ â€¢ Kernel launch     â”‚    â”‚ â€¢ Kernel launch     â”‚    â”‚
â”‚  â”‚ â€¢ Context mgmt      â”‚    â”‚ â€¢ Context mgmt      â”‚    â”‚ â€¢ Context mgmt      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              HARDWARE LAYER                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚    NVIDIA GPU       â”‚    â”‚      AMD GPU        â”‚                               â”‚
â”‚  â”‚                     â”‚    â”‚                     â”‚                               â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â”‚   SM (Cores)    â”‚ â”‚    â”‚ â”‚   CU (Cores)    â”‚ â”‚    â”‚   Intel Xe Cores    â”‚    â”‚
â”‚  â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚    â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚ â”‚ â”‚FP32 | INT32 â”‚ â”‚ â”‚    â”‚ â”‚ â”‚FP32 | INT32 â”‚ â”‚ â”‚    â”‚ â”‚  Vector Engines â”‚ â”‚    â”‚
â”‚  â”‚ â”‚ â”‚FP64 | BF16  â”‚ â”‚ â”‚    â”‚ â”‚ â”‚FP64 | BF16  â”‚ â”‚ â”‚    â”‚ â”‚  Matrix Engines â”‚ â”‚    â”‚
â”‚  â”‚ â”‚ â”‚Tensor Cores â”‚ â”‚ â”‚    â”‚ â”‚ â”‚Matrix Cores â”‚ â”‚ â”‚    â”‚ â”‚  Ray Tracing    â”‚ â”‚    â”‚
â”‚  â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚    â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”‚                     â”‚    â”‚                     â”‚                               â”‚
â”‚  â”‚ Memory Hierarchy:   â”‚    â”‚ Memory Hierarchy:   â”‚    Memory Hierarchy:          â”‚
â”‚  â”‚ â€¢ L1 Cache (KB)     â”‚    â”‚ â€¢ L1 Cache (KB)     â”‚    â€¢ L1 Cache                 â”‚
â”‚  â”‚ â€¢ L2 Cache (MB)     â”‚    â”‚ â€¢ L2 Cache (MB)     â”‚    â€¢ L2 Cache                 â”‚
â”‚  â”‚ â€¢ Global Mem (GB)   â”‚    â”‚ â€¢ Global Mem (GB)   â”‚    â€¢ Global Memory            â”‚
â”‚  â”‚ â€¢ Shared Memory     â”‚    â”‚ â€¢ LDS (Local Data   â”‚    â€¢ Shared Local Memory      â”‚
â”‚  â”‚ â€¢ Constant Memory   â”‚    â”‚   Store)            â”‚    â€¢ Constant Memory          â”‚
â”‚  â”‚ â€¢ Texture Memory    â”‚    â”‚ â€¢ Constant Memory   â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Compilation Pipeline Deep Dive

#### 1. **Source Code â†’ Frontend Parsing**
- **CUDA**: NVCC separates host (CPU) and device (GPU) code, parses CUDA extensions
- **HIP**: Clang-based compiler with HIP runtime API that maps to either CUDA or ROCm
- **OpenCL/SYCL**: LLVM-based compilation with cross-platform intermediate representation

#### 2. **Frontend â†’ Intermediate Representation**
```
High-Level Code                    Intermediate Form
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
__global__ void kernel()    â†’     PTX (NVIDIA)
{                                 GCN Assembly (AMD)  
    int id = threadIdx.x;         SPIR-V (OpenCL/Vulkan)
    output[id] = input[id] * 2;   LLVM IR (SYCL)
}
```

#### 3. **Runtime Compilation & Optimization**
- **NVIDIA**: PTX â†’ SASS (GPU-specific machine code) via JIT compilation
- **AMD**: GCN Assembly â†’ GPU microcode via ROCm runtime
- **Optimizations**: Register allocation, memory coalescing, instruction scheduling

#### 4. **Hardware Execution Model**

| Abstraction Level | NVIDIA Term | AMD Term | Description |
|------------------|-------------|----------|-------------|
| **Thread** | Thread | Work-item | Single execution unit |
| **Thread Group** | Warp (32 threads) | Wavefront (64 threads) | SIMD execution group |
| **Thread Block** | Block | Work-group | Shared memory + synchronization |
| **Grid** | Grid | NDRange | Collection of all thread blocks |

#### 5. **Memory Architecture Mapping**

```
Programming Model              Hardware Implementation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Global Memory        â†’         GPU DRAM (HBM/GDDR)
Shared Memory        â†’         On-chip SRAM (48-164KB per SM/CU)
Local Memory         â†’         GPU DRAM (spilled registers)
Constant Memory      â†’         Cached read-only GPU DRAM
Texture Memory       â†’         Cached GPU DRAM with interpolation
Registers            â†’         On-chip register file (32K-64K per SM/CU)
```

### Performance Implications

Understanding this architecture helps optimize GPU code:

1. **Memory Coalescing**: Access patterns that align with hardware memory buses
2. **Occupancy**: Balancing registers, shared memory, and thread blocks per SM/CU
3. **Divergence**: Minimizing different execution paths within warps/wavefronts
4. **Latency Hiding**: Using enough threads to hide memory access latency
5. **Memory Hierarchy**: Optimal use of each memory type based on access patterns

This architectural knowledge is essential for writing efficient GPU code and is covered progressively throughout our modules.

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ¯ **Complete Curriculum** | 9 progressive modules from basics to advanced topics |
| ğŸ’» **Cross-Platform** | Full CUDA and HIP support for NVIDIA and AMD GPUs |
| ğŸ³ **Docker Ready** | Complete containerized development environment with CUDA 12.9.1 & ROCm 7.0 |
| ğŸ”§ **Production Quality** | Professional build systems, auto-detection, testing, and profiling |
| ğŸ“Š **Performance Focus** | Optimization techniques and benchmarking throughout |
| ğŸŒ **Community Driven** | Open source with comprehensive contribution guidelines |
| ğŸ§ª **Advanced Libraries** | Support for Thrust, MIOpen, and production ML frameworks |

## ğŸš€ Quick Start

### Option 1: Docker (Recommended)
Get started immediately without installing CUDA/ROCm on your host system:

```bash
# Clone the repository
git clone https://github.com/AIComputing101/gpu-programming-101.git
cd gpu-programming-101

# Auto-detect your GPU and start development environment
./docker/scripts/run.sh --auto

# Inside container: verify GPU access and start learning
/workspace/test-gpu.sh
cd modules/module1 && make && ./build/01_vector_addition_cuda
```

### Option 2: Native Installation
For direct system installation:

```bash
# Prerequisites: CUDA 12.0+ or ROCm 7.0+, GCC 9+, Make

# Clone and build
git clone https://github.com/AIComputing101/gpu-programming-101.git
cd gpu-programming-101

# Verify your setup
make check-system

# Build and run first example
make module1
cd modules/module1/examples
./01_vector_addition_cuda
```

## ğŸ¯ Learning Path

Choose your track based on your experience level:

- **ğŸ‘¶ Beginner Track** (Modules 1-3) - GPU fundamentals, memory management, first kernels
- **ğŸ”¥ Intermediate Track** (Modules 4-5) - Advanced programming, performance optimization  
- **ğŸš€ Advanced Track** (Modules 6-9) - Parallel algorithms, domain applications, production deployment

*Each track builds on the previous one, so start with the appropriate level for your background.*

## ğŸ“š Modules

Our comprehensive curriculum progresses from fundamental concepts to production-ready optimization techniques:

| Module | Level | Duration | Focus Area | Key Topics | Examples |
|--------|-------|----------|------------|------------|----------|
| [**Module 1**](modules/module1/) | ğŸ‘¶ Beginner | 4-6h | **GPU Fundamentals** | Architecture, Memory, First Kernels | 13 |
| [**Module 2**](modules/module2/) | ğŸ‘¶â†’ğŸ”¥ | 6-8h | **Memory Optimization** | Coalescing, Shared Memory, Texture | 10 |
| [**Module 3**](modules/module3/) | ğŸ”¥ Intermediate | 6-8h | **Execution Models** | Warps, Occupancy, Synchronization | 12 |
| [**Module 4**](modules/module4/) | ğŸ”¥â†’ğŸš€ | 8-10h | **Advanced Programming** | Streams, Multi-GPU, Unified Memory | 9 |
| [**Module 5**](modules/module5/) | ğŸš€ Advanced | 6-8h | **Performance Engineering** | Profiling, Bottleneck Analysis | 5 |
| [**Module 6**](modules/module6/) | ğŸš€ Advanced | 8-10h | **Parallel Algorithms** | Reduction, Scan, Convolution | 10 |
| [**Module 7**](modules/module7/) | ğŸš€ Expert | 8-10h | **Algorithmic Patterns** | Sorting, Graph Algorithms | 4 |
| [**Module 8**](modules/module8/) | ğŸš€ Expert | 10-12h | **Domain Applications** | ML, Scientific Computing | 4 |
| [**Module 9**](modules/module9/) | ğŸš€ Expert | 6-8h | **Production Deployment** | Libraries, Integration, Scaling | 4 |

**ğŸ“ˆ Progressive Learning Path: 71 Examples â€¢ 50+ Hours â€¢ Beginner to Expert**

### Learning Progression

```
Module 1: Hello GPU World          Module 6: Parallel Algorithms
    â†“                                 â†“
Module 2: Memory Mastery          Module 7: Advanced Patterns  
    â†“                                 â†“
Module 3: Execution Deep Dive     Module 8: Real Applications
    â†“                                 â†“
Module 4: Advanced Features       Module 9: Production Ready
    â†“                             
Module 5: Performance Tuning     
```

**[ğŸ“š View All Modules â†’](modules/)**

## ğŸ› ï¸ Prerequisites

### Hardware Requirements

#### NVIDIA GPU Systems
- **Minimum GPU**: GTX 1060 6GB, GTX 1650, RTX 2060 or better
- **Recommended GPU**: RTX 3070/4070 (12GB+), RTX 3080/4080 (16GB+) 
- **Professional/Advanced**: RTX 4090 (24GB), RTX A6000 (48GB), Tesla/Quadro series
- **Architecture Support**: Maxwell, Pascal, Volta, Turing, Ampere, Ada Lovelace, Hopper
- **Compute Capability**: 5.0+ (Maxwell architecture or newer)

#### AMD GPU Systems  
- **Minimum GPU**: RX 580 8GB, RX 6600, RX 7600 or better
- **Recommended GPU**: RX 6700 XT/7700 XT (12GB+), RX 6800 XT/7800 XT (16GB+)
- **Professional/Advanced**: RX 7900 XTX (24GB), Radeon PRO W7800 (48GB), Instinct MI series
- **Architecture Support**: RDNA2, RDNA3, RDNA4, GCN 5.0+, CDNA series
- **ROCm Compatibility**: Officially supported AMD GPUs only

#### System Memory & CPU
- **Minimum RAM**: 16GB system RAM 
- **Recommended RAM**: 32GB+ for advanced modules and multi-GPU setups
- **Professional Setup**: 64GB+ for large-scale scientific computing
- **CPU Requirements**: 
  - **Intel**: Haswell (2013) or newer for PCIe atomics support
  - **AMD**: Zen 1 (2017) or newer for PCIe atomics support
- **Storage**: 20GB+ free space for Docker containers and examples

### Software Requirements

#### Operating System Support
- **Linux** (Recommended): Ubuntu 22.04/24.04 LTS, RHEL 8/9, SLES 15 SP5
- **Windows**: Windows 10/11 with WSL2 recommended for optimal compatibility
- **macOS**: macOS 12+ (Metal Performance Shaders for basic GPU compute)

#### GPU Computing Platforms
- **CUDA Toolkit**: 12.0+ (Docker uses CUDA 12.9.1)
  - **Driver Requirements**: 
    - Linux: 550.54.14+ for CUDA 12.4+
    - Windows: 551.61+ for CUDA 12.4+
- **ROCm Platform**: 7.0+ (Docker uses ROCm 7.0)
  - **Driver Requirements**: Latest AMDGPU-PRO or open-source AMDGPU drivers
  - **Kernel Support**: Linux kernel 5.4+ recommended

#### Development Environment
- **Compilers**:
  - **GCC**: 9.0+ (GCC 11+ recommended for C++17 features)
  - **Clang**: 10.0+ (Clang 14+ recommended)
  - **MSVC**: 2019+ (2022 17.10+ for CUDA 12.4+ support)
- **Build Tools**: Make 4.0+, CMake 3.18+ (optional)
- **Docker**: 20.10+ with GPU runtime support (nvidia-container-toolkit or ROCm containers)

#### Additional Tools (Included in Docker)
- **Profiling**: Nsight Compute, Nsight Systems (NVIDIA), rocprof (AMD)
- **Debugging**: cuda-gdb, rocgdb, compute-sanitizer
- **Libraries**: cuBLAS, cuFFT, rocBLAS, rocFFT (for advanced modules)
- **ML Libraries**: Thrust (NVIDIA), MIOpen (AMD) for deep learning applications
- **System Management**: NVML (NVIDIA), ROCm SMI (AMD) for hardware monitoring

### Performance Expectations by Hardware Tier

| Hardware Tier | Example GPUs | VRAM | Expected Performance | Suitable Modules |
|---------------|--------------|------|---------------------|------------------|
| **Entry Level** | GTX 1060 6GB, RX 580 8GB | 6-8GB | 10-50x CPU speedup | Modules 1-3 |
| **Mid-Range** | RTX 3060 Ti, RX 6700 XT | 12GB | 50-200x CPU speedup | Modules 1-6 |
| **High-End** | RTX 4070 Ti, RX 7800 XT | 16GB | 100-500x CPU speedup | All modules |
| **Professional** | RTX 4090, RX 7900 XTX | 24GB | 200-1000x+ CPU speedup | All modules + research |

### Programming Knowledge
- **C/C++**: Intermediate level (pointers, memory management, basic templates)
- **Parallel Programming**: Basic understanding of threads and synchronization helpful
- **Command Line**: Comfortable with terminal/shell operations
- **Mathematics**: Linear algebra and calculus basics beneficial for advanced modules
- **Version Control**: Basic Git knowledge for contributing

### Network Requirements (Docker Setup)
- **Internet Connection**: Required for initial Docker image downloads (~8GB total)
- **Bandwidth**: 50+ Mbps recommended for efficient container downloads
- **Storage**: Additional 20GB for Docker images and build cache

## ğŸ³ Docker Development

Experience the full development environment with zero setup:

```bash
# Build development containers
./docker/scripts/build.sh --all

# Start interactive development
./docker/scripts/run.sh cuda    # For NVIDIA GPUs
./docker/scripts/run.sh rocm    # For AMD GPUs
./docker/scripts/run.sh --auto  # Auto-detect GPU type
```

**Docker Benefits:**
- ğŸ¯ Zero host configuration required
- ğŸ”§ Complete development environment (compilers, debuggers, profilers)
- ğŸŒ Cross-platform testing (test your code on both CUDA and HIP)
- ğŸ“¦ Isolated and reproducible builds
- ğŸ§¹ Easy cleanup when done

**Container Specifications:**
- **CUDA**: NVIDIA CUDA 12.9.1 on Ubuntu 22.04
- **ROCm**: AMD ROCm 7.0 on Ubuntu 24.04 
- **Libraries**: Production-ready toolchains with debugging support

**[ğŸ“– Complete Docker Guide â†’](docker/README.md)**

## ğŸ”§ Build System

Our advanced build system features automatic GPU vendor detection and optimized configurations:

### Project-Wide Commands
```bash
make all           # Build all modules with auto-detection
make test          # Run comprehensive tests  
make clean         # Clean all artifacts
make check-system  # Verify GPU setup and dependencies
make status        # Show module completion status
```

### Module-Specific Commands
```bash
cd modules/module1/examples
make               # Build all examples with vendor auto-detection
make test          # Run module tests
make profile       # Performance profiling
make debug         # Debug builds with extra checks
```

### Advanced Build Features
- **Automatic GPU Detection**: Detects NVIDIA/AMD hardware and builds accordingly
- **Production Optimization**: `-O3`, fast math, architecture-specific optimizations
- **Debug Support**: Full debugging symbols and validation checks
- **Library Management**: Automatic detection of optional dependencies (NVML, MIOpen)
- **Cross-Platform**: Single Makefile supports both CUDA and HIP builds

##  Performance Expectations

| Module Level | Typical GPU Speedup | Memory Efficiency | Code Quality |
|--------------|-------------------|------------------|--------------|
| **Beginner** | 10-100x | 60-80% | Educational |
| **Intermediate** | 50-500x | 80-95% | Optimized |
| **Advanced** | 100-1000x | 85-95% | Production |
| **Expert** | 500-5000x | 95%+ | Library-Quality |

## ğŸ› Troubleshooting

### Common Issues & Solutions

**GPU Not Detected**
```bash
# NVIDIA
nvidia-smi  # Should show your GPU
export PATH=/usr/local/cuda/bin:$PATH

# AMD  
rocm-smi   # Should show your GPU
export HIP_PLATFORM=amd
```

**Compilation Errors**
```bash
# Check CUDA installation
nvcc --version
make check-cuda

# Check HIP installation  
hipcc --version
make check-hip
```

**Docker Issues**
```bash
# Test Docker GPU access
./docker/scripts/test.sh

# Rebuild containers
./docker/scripts/build.sh --clean --all
```

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| **README.md** | Main project documentation and getting started guide |
| [**CONTRIBUTING.md**](CONTRIBUTING.md) | How to contribute to the project |
| [**Docker Guide**](docker/README.md) | Complete Docker setup and usage |
| [**Module READMEs**](modules/) | Individual module documentation |

## ğŸ¤ Contributing

We welcome contributions from the community! This project thrives on:

- ğŸ“ **New Examples**: Implementing additional GPU algorithms
- ğŸ› **Bug Fixes**: Improving existing code and documentation  
- ğŸ“š **Documentation**: Enhancing explanations and tutorials
- ğŸ”§ **Optimizations**: Performance improvements and best practices
- ğŸŒ **Platform Support**: Cross-platform compatibility improvements

**[ğŸ“– Contributing Guidelines â†’](CONTRIBUTING.md)** â€¢ **[ğŸ› Report Issues â†’](https://github.com/AIComputing101/gpu-programming-101/issues)** â€¢ **[ğŸ’¡ Request Features â†’](https://github.com/AIComputing101/gpu-programming-101/issues/new?template=feature_request.md)**

## ğŸ† Community & Support

- ğŸŒŸ **Star this project** if you find it helpful!
- ğŸ› **Report bugs** using our [issue templates](https://github.com/AIComputing101/gpu-programming-101/issues/new/choose)
- ğŸ’¬ **Join discussions** in [GitHub Discussions](https://github.com/AIComputing101/gpu-programming-101/discussions)
- ğŸ“§ **Get help** from the community and maintainers

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Citation

If you use this project in your research, education, or publications, please cite it as:

### BibTeX
```bibtex
@misc{gpu-programming-101,
  title={GPU Programming 101: A Comprehensive Educational Project for CUDA and HIP},
  author={{Stephen Shao}},
  year={2025},
  howpublished={\url{https://github.com/AIComputing101/gpu-programming-101}},
  note={A complete GPU programming educational resource with 70+ production-ready examples covering fundamentals through advanced optimization techniques for NVIDIA CUDA and AMD HIP platforms}
}
```

### IEEE Format
Stephen Shao, "GPU Programming 101: A Comprehensive Educational Project for CUDA and HIP," GitHub, 2025. [Online]. Available: https://github.com/AIComputing101/gpu-programming-101

## ğŸ™ Acknowledgments

- ğŸ¯ **NVIDIA** and **AMD** for excellent GPU computing ecosystems
- ğŸ“š **GPU computing community** for sharing knowledge and best practices  
- ğŸ‘¥ **Contributors** who make this project better every day

---

**Ready to unlock the power of GPU computing?**

**[ğŸš€ Get Started Now](#-quick-start)** â€¢ **[ğŸ“š View Modules](modules/)** â€¢ **[ğŸ³ Try Docker](docker/README.md)**

---

**â­ Star this project â€¢ ğŸ´ Fork and contribute â€¢ ğŸ“¢ Share with others**

*Built with â¤ï¸ for the AI Computing 101*
